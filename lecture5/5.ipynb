{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "n3JilpUrTIYZ",
    "outputId": "720acec4-14a4-4372-b811-8491e03b1b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-09 02:26:39--  http://model.scir.yunfutech.com/model/ltp_data_v3.4.0.zip\n",
      "Resolving model.scir.yunfutech.com (model.scir.yunfutech.com)... 47.89.64.233, 47.89.64.202, 47.89.64.204\n",
      "Connecting to model.scir.yunfutech.com (model.scir.yunfutech.com)|47.89.64.233|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 637993278 (608M) [application/zip]\n",
      "Saving to: ‘ltp_data_v3.4.0.zip’\n",
      "\n",
      "ltp_data_v3.4.0.zip 100%[===================>] 608.44M  98.2MB/s    in 6.6s    \n",
      "\n",
      "2020-06-09 02:26:46 (92.6 MB/s) - ‘ltp_data_v3.4.0.zip’ saved [637993278/637993278]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget http://model.scir.yunfutech.com/model/ltp_data_v3.4.0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "FHo4AlAaTLoH",
    "outputId": "f857c1a0-a962-4bab-e836-6d81dc64f663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ltp_data_v3.4.0.zip\n",
      "   creating: ltp_data_v3.4.0/\n",
      "  inflating: ltp_data_v3.4.0/cws.model  \n",
      "  inflating: ltp_data_v3.4.0/md5.txt  \n",
      "  inflating: ltp_data_v3.4.0/ner.model  \n",
      "  inflating: ltp_data_v3.4.0/parser.model  \n",
      "  inflating: ltp_data_v3.4.0/pisrl.model  \n",
      "  inflating: ltp_data_v3.4.0/pos.model  \n",
      "  inflating: ltp_data_v3.4.0/version  \n"
     ]
    }
   ],
   "source": [
    "!unzip ltp_data_v3.4.0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "colab_type": "code",
    "id": "YPwFV0vdac8_",
    "outputId": "8e688ad8-4e92-48ab-e039-b8ff2dd87070"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyltp\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/aa/72/2d88c54618cf4d8916832950374a6f265e12289fa9870aeb340800a28a62/pyltp-0.2.1.tar.gz (5.3MB)\n",
      "\u001B[K     |████████████████████████████████| 5.3MB 27kB/s \n",
      "\u001B[?25hBuilding wheels for collected packages: pyltp\n"
     ]
    }
   ],
   "source": [
    "!pip install pyltp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XQPsZZ4zaKuv"
   },
   "outputs": [],
   "source": [
    "!tar -zxvf AutoMaster_TrainSet.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IGuA2CBpP6x5"
   },
   "outputs": [],
   "source": [
    "!tar -zxvf ckpt.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CbF1PubS2n9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pyltp import SentenceSplitter,NamedEntityRecognizer,Postagger,Parser,Segmentor\n",
    "import pandas as pd\n",
    "from gensim.models.word2vec import LineSentence, Word2Vec\n",
    "import time, shutil, os\n",
    "print(tf.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "# shutil.rmtree('checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bGUCcT_HS2oB"
   },
   "outputs": [],
   "source": [
    "vocabulary_dimension = 100\n",
    "gru_unites = 128\n",
    "attn_units = 32\n",
    "batch_size = 128\n",
    "learning_rate = 0.01\n",
    "checkpoint_dir = './checkpoint'\n",
    "EPOCHS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3xEzXL61S2oE"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./AutoMaster_TrainSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TNweMNQKS2oG"
   },
   "outputs": [],
   "source": [
    "cws_model = \"./ltp_data_v3.4.0/cws.model\"\n",
    "\n",
    "def get_word_list(sentence=None,sentences=None,model=cws_model):\n",
    "    #得到分词\n",
    "    segmentor = Segmentor()\n",
    "    segmentor.load(model)\n",
    "    if sentences is not None:\n",
    "        for i, s in enumerate(sentences):\n",
    "            sentences[i] = list(segmentor.segment(s))\n",
    "        return sentences\n",
    "    else:\n",
    "        word_list = list(segmentor.segment(sentence))\n",
    "    segmentor.release()\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lo3WhngzS2oJ"
   },
   "outputs": [],
   "source": [
    "inp = list(df.Question)\n",
    "outp = list(df.Report)\n",
    "assert len(inp) == len(outp)\n",
    "\n",
    "# 去除随时联系, 查阅资料等无意义回答\n",
    "drop_index_set = set()\n",
    "for index, (i, o) in enumerate(zip(inp, outp)):\n",
    "    if not type(i) == type(o) == str or len(i) < 8 or len(o) < 8:\n",
    "        drop_index_set.add(index)\n",
    "\n",
    "inp = [el for i, el in enumerate(inp) if i not in drop_index_set]\n",
    "outp = [el for i, el in enumerate(outp) if i not in drop_index_set]\n",
    "\n",
    "inp = get_word_list(sentences=inp)\n",
    "outp = get_word_list(sentences=outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fxuRgNlNS2oL"
   },
   "outputs": [],
   "source": [
    "def get_length(tensor):\n",
    "    length = sorted([len(t) for t in tensor])\n",
    "    return length[int(len(length) * 0.95)] + 2\n",
    "\n",
    "inp_length = get_length(inp)\n",
    "outp_length = get_length(outp)\n",
    "\n",
    "with open('../gensim_input.txt', 'w') as f:\n",
    "    for s in inp:\n",
    "        s = ['<start>'] + s + ['<end>']\n",
    "        f.write(' '.join(s) + '\\n')\n",
    "    for s in outp:\n",
    "        s = ['<start>'] + s + ['<end>']\n",
    "        f.write(' '.join(s) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tK2OyuSsS2oN",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: 10: No such file or directory\r\n",
      "==> ../gensim_input.txt <==\r\n",
      "<start> 2010 款 宝马 X1 ， 2011年 出厂 ， 2.0 排量 ， 通用 6L45 变速箱 ， 原地 换 挡 位 PRND 车辆 闯动 ， 行驶 升降档 正常 ， 4 轮 离 地 换 挡 无 冲击感 ， 更换 变速箱 油 12L 无 改变 。 试过 一 辆 2014年 进口 X1 原地 换 挡位 也 有 冲击感 ， 这 是 什么 情况 ， 哪里 的 问题 <end>\r\n",
      "<start> 3.0V6 发动机 号 在 什么 位置 ， 有 照片 最 好 ！ <end>\r\n",
      "<start> 2012款 奔驰 c180 怎么样 ， 维修 保养 ， 动力 ， 值得 拥有 吗 <end>\r\n",
      "<start> 科鲁兹 变速箱 旁边 漏 机油 <end>\r\n",
      "<start> 我 要 怎么 才 能 知道 车子 断开 电瓶 电源 之后 要 不 要 做 节气门 或 防盗 ， 音响 等 的 重置 ， 还有 节气门 重 置 是 不 是 所有 车型 都 一样 ， 打开 电源 不 启动 车子 ， 油门 踩 到底 五 秒 ， 重复 几 次 就 可以 了 。 其他 设备 要 密码 才 行 是 不 是 <end>\r\n",
      "<start> 昌河 Q35 音响 怎么 拆 装 <end>\r\n",
      "<start> 长安 35 朝阳 轮胎 不 要 里面 的 钢圈 。 用 我 自己 的 钢圈 多少 钱 外面 换 多少 钱 <end>\r\n",
      "<start> 吉利 远景 外 球笼 上 那个 大 螺丝 是 顺时针 拧 下来 还是 反 的 <end>\r\n",
      "<start> 五菱 之 光 6376nf ， 自己 装 了 铁将军 防盗器 ， 五线马达 的 白褐 两 根 信号线 接到 什么 地方 去 ？ 我 想 实现 一 控 三马达 <end>\r\n",
      "<start> 丰田 花冠 行驶 了 十万 公里 皮带 要 换 吗 <end>\r\n"
     ]
    }
   ],
   "source": [
    "! head 10 ../gensim_input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XRabZtVFS2oQ"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus_file='../gensim_input.txt', size=vocabulary_dimension, min_count=10, workers=8, sg=0, negative=5, iter=5)\n",
    "w2v_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NsMHGNW86ojQ"
   },
   "outputs": [],
   "source": [
    "train_x_vocab = []\n",
    "for s in inp:\n",
    "    s = s[:inp_length - 2]\n",
    "    s = ['<start>'] + s + ['<end>']\n",
    "    s = s + ['<pad>'] * (inp_length - len(s))\n",
    "    train_x_vocab.append(s)\n",
    "\n",
    "train_y_vocab = []\n",
    "for s in outp:\n",
    "    s = s[:outp_length - 2]\n",
    "    s = ['<start>'] + s + ['<end>']\n",
    "    s = s + ['<pad>'] * (inp_length - len(s))\n",
    "    train_y_vocab.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ByfsY8DbAYbO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9045"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xkK8OWg26ojS"
   },
   "outputs": [],
   "source": [
    "model_vocab_size = len(model.wv.vocab)\n",
    "\n",
    "inp_vocabulary_matrix = np.array([model.wv[model.wv.index2word[i]] for i in range(model_vocab_size)])\n",
    "\n",
    "model.wv['<pad>'] = np.zeros((vocabulary_dimension,))\n",
    "model.wv['<unknown>'] = np.average(inp_vocabulary_matrix, axis=0)\n",
    "\n",
    "pad_wv_index = model.wv.vocab['<pad>'].index\n",
    "unknown_wv_index = model.wv.vocab['<unknown>'].index\n",
    "\n",
    "model_vocab_size = len(model.wv.vocab)\n",
    "inp_vocabulary_matrix = [model.wv[model.wv.index2word[i]] for i in range(model_vocab_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7sRxDGVcAt5Y"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unknown>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index2word[9046]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "al7duhEL6ojU"
   },
   "outputs": [],
   "source": [
    "train_x_token = []\n",
    "train_x_extended_token = []\n",
    "train_x_pad_mask = []\n",
    "train_x_oov_token = []\n",
    "train_x_oov_len = []\n",
    "train_y_token = []\n",
    "train_y_pad_mask = []\n",
    "\n",
    "for s in train_x_vocab:\n",
    "    s_word_index = []\n",
    "    s_oov_index = []\n",
    "    s_oov_dict = {}\n",
    "    pad_mask = [1 for _ in range(len(s))]\n",
    "    for i, w in enumerate(s):\n",
    "        if w == '<unknown>':\n",
    "            raise ValueError\n",
    "        elif w == '<pad>':\n",
    "            pad_mask[i] = 0\n",
    "            s_word_index.append(pad_wv_index)\n",
    "            s_oov_index.append(pad_wv_index)\n",
    "        elif w in model.wv:\n",
    "            s_word_index.append(model.wv.vocab[w].index)\n",
    "            s_oov_index.append(model.wv.vocab[w].index)\n",
    "        else:\n",
    "            s_word_index.append(unknown_wv_index)\n",
    "            if w not in s_oov_dict:\n",
    "                s_oov_dict[w] = len(s_oov_dict)\n",
    "            s_oov_index.append(model_vocab_size + s_oov_dict[w])\n",
    "    \n",
    "    train_x_token.append(s_word_index)\n",
    "    train_x_extended_token.append(s_oov_index)\n",
    "    train_x_pad_mask.append(pad_mask)\n",
    "    train_x_oov_token.append(s_oov_dict)\n",
    "    \n",
    "train_x_oov_token = [list(el.keys()) for el in train_x_oov_token]\n",
    "train_x_oov_len = [len(el) for el in train_x_oov_token]\n",
    "\n",
    "for s in train_y_vocab:\n",
    "    s_word_index = []\n",
    "    pad_mask = [1 for _ in range(len(s))]\n",
    "    for i, w in enumerate(s):\n",
    "        if w == '<unknown>':\n",
    "            raise ValueError\n",
    "        elif w == '<pad>':\n",
    "            pad_mask[i] = 0\n",
    "            s_word_index.append(pad_wv_index)\n",
    "        elif w in model.wv:\n",
    "            s_word_index.append(model.wv.vocab[w].index)\n",
    "        else:\n",
    "            s_word_index.append(unknown_wv_index)\n",
    "    train_y_token.append(s_word_index)\n",
    "    train_y_pad_mask.append(pad_mask)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UIvoOsWW6ojZ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_test_split, train_input shape: (74728, 86)\n",
      "train_test_split, test_input shape: (3934, 86)\n"
     ]
    }
   ],
   "source": [
    "input_train, input_test, extended_input_train, extended_input_test, input_pad_mask_train, input_pad_mask_test, \\\n",
    "    input_oov_train_dict, input_oov_test_dict, input_oov_train_len, input_oov_test_len, output_train, output_test, \\\n",
    "    output_pad_mask_train, output_pad_mask_test = train_test_split(train_x_token, train_x_extended_token, train_x_pad_mask,\n",
    "                                                                   train_x_oov_token, train_x_oov_len,train_y_token,train_y_pad_mask,\n",
    "                                                                   test_size=0.05, random_state=6)\n",
    "train_dataset_len = len(input_train)\n",
    "test_dataset_len = len(input_test)\n",
    "\n",
    "input_train, input_test, extended_input_train, extended_input_test, input_pad_mask_train, input_pad_mask_test, \\\n",
    "input_oov_train_len, input_oov_test_len, output_train, output_test, output_pad_mask_train, output_pad_mask_test = \\\n",
    "tf.convert_to_tensor(input_train), tf.convert_to_tensor(input_test), \\\n",
    "tf.convert_to_tensor(extended_input_train), tf.convert_to_tensor(extended_input_test), \\\n",
    "tf.convert_to_tensor(input_pad_mask_train), tf.convert_to_tensor(input_pad_mask_test), \\\n",
    "tf.convert_to_tensor(input_oov_train_len), tf.convert_to_tensor(input_oov_test_len), \\\n",
    "tf.convert_to_tensor(output_train), tf.convert_to_tensor(output_test), \\\n",
    "tf.convert_to_tensor(output_pad_mask_train), tf.convert_to_tensor(output_pad_mask_test)\n",
    "\n",
    "print('train_test_split, train_input shape:', input_train.shape)\n",
    "print('train_test_split, test_input shape:', input_test.shape)\n",
    "\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices(\n",
    "(input_train, extended_input_train, input_pad_mask_train, input_oov_train_len, output_train, output_pad_mask_train))\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices(\n",
    "(input_test, extended_input_test, input_pad_mask_test, input_oov_test_len, output_test, output_pad_mask_test))\n",
    "dataset_train_batch = dataset_train.batch(batch_size=batch_size, drop_remainder=True)\n",
    "dataset_test_batch = dataset_test.batch(batch_size=batch_size, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q4bvUWOe7I-b"
   },
   "outputs": [],
   "source": [
    "dataset_train_batch, dataset_test_batch, dataset_train_len, dataset_test_len, dataset_train_oov_dict, \\\n",
    "    dataset_test_oov_dict = dataset_train_batch, dataset_test_batch, train_dataset_len, test_dataset_len, \\\n",
    "input_oov_train_dict, input_oov_test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2tTevdKL7I-d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((128, 86), (128, 86), (128, 86), (128,), (128, 86), (128, 86)), types: (tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mDa0BHXi7I-e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = PGN(gru_unites, attn_units, batch_size, np.array(inp_vocabulary_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQm5B6Vd7I-g"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.05) # , clipvalue=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SOJpaXA17I-i"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fda171cdd68>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_prefix = os.path.join('./', \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=model.encoder, attention=model.attention,\n",
    "                                 decoder=model.decoder, pointer=model.pointer)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HN68pZyo7I-k"
   },
   "outputs": [],
   "source": [
    "dataset_len = input_train.shape[0]\n",
    "batch_sz = batch_size\n",
    "steps_per_epoch = dataset_len // batch_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4tbzibRj7I-m"
   },
   "outputs": [],
   "source": [
    "def train_one_batch(mode, w2v_model, model, optimizer, oov_dict, st, inp, targ, enc_extended_inp, enc_pad_mask,\n",
    "                    batch_oov_len, cov_loss_wt, padding_mask=None):\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = model.call_encoder(inp)\n",
    "        final_dist, attentions, coverages, _, _, _ = model(targ, enc_extended_inp, enc_pad_mask, batch_oov_len,\n",
    "                                                enc_output, enc_hidden, use_coverage=True,prev_coverage=None)\n",
    "        loss = loss_function(targ,final_dist,padding_mask)+cov_loss_wt*coverage_loss(attentions,coverages,padding_mask)\n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        variables = model.encoder.trainable_variables + model.attention.trainable_variables + \\\n",
    "                        model.decoder.trainable_variables + model.pointer.trainable_variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FJOgWzPK7I-p"
   },
   "outputs": [],
   "source": [
    "save_chkp_epoch = 1\n",
    "train_epoch = EPOCHS\n",
    "cov_loss_wt = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gzCHCmSVGbwV"
   },
   "outputs": [],
   "source": [
    "mode, w2v_model, model, optimizer, checkpoint, checkpoint_prefix, save_chkp_epoch,\\\n",
    "dataset_batch, dataset_len, dataset_oov_dict, batch_sz, EPOCHS, cov_loss_wt = 'train', w2v_model, model, \\\n",
    "optimizer, checkpoint, checkpoint_prefix, save_chkp_epoch,\\\n",
    "dataset_train_batch, dataset_train_len, dataset_train_oov_dict, batch_sz, train_epoch, cov_loss_wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fMsiWi_T7I-q",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-29-e7ce67df9d32>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0menc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menc_extend\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menc_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menc_oov_len\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdec\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdec_mask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset_train_batch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtake\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msteps_per_epoch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0mst\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mbatch\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mbatch_sz\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m         \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m         batch_loss = train_one_batch('train',w2v_model,model,optimizer,dataset_oov_dict,st,enc,dec,enc_extend,enc_mask,\n\u001B[1;32m      8\u001B[0m                                          enc_oov_len, cov_loss_wt=cov_loss_wt, padding_mask=dec_mask)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    for (batch, (enc, enc_extend, enc_mask, enc_oov_len, dec, dec_mask)) in enumerate(dataset_train_batch.take(steps_per_epoch)):\n",
    "        st = (batch + 1) * batch_sz\n",
    "        raise\n",
    "        batch_loss = train_one_batch('train',w2v_model,model,optimizer,dataset_oov_dict,st,enc,dec,enc_extend,enc_mask,\n",
    "                                         enc_oov_len, cov_loss_wt=cov_loss_wt, padding_mask=dec_mask)\n",
    "        total_loss += batch_loss\n",
    "        if batch % 500 == 0:\n",
    "                print('$$$$$$$$Epoch {} Batch {} Loss {:.4f}$$$$$$$$'.format(epoch, batch, batch_loss.numpy()))\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    learning_rate *= 0.8\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipvalue=5)\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc, enc_extend, enc_mask, enc_oov_len, dec, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unknown>'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.index2word[9046]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 86), dtype=int32, numpy=\n",
       "array([[   2,   24,   55, 2397,  838,   37,  154, 3024,  100, 1839,   42,\n",
       "         689,    4,   23,   31,    9,    3, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045],\n",
       "       [   2, 2711,  611,  190,    0, 1369,   38,    0,  320,  286,  237,\n",
       "        9046,  931,  142,    0,  132,  419,   72, 9046,  354,    0,  321,\n",
       "        3069,    5,  320,   17,    5, 3544,   37,  488, 5135, 2676,    5,\n",
       "         272, 1652,    6,    9,    3, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045],\n",
       "       [   2,   48,  145,    0,  617,   47,  801,  611,    1,    0,   12,\n",
       "         386,    6,   24, 4455,    0,  340,  205,   12,  356,    0,  221,\n",
       "        9046, 1404,   24,  103,    0,  342,  362,   88, 9046,  430,  179,\n",
       "        9046,    4,   39,   85,   93,    9,  669,    7,  204,    4,  616,\n",
       "           0,   72,  352, 1250,  619,    0,  170,  145,    3, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045],\n",
       "       [   2, 1437, 9046,  134, 9046,  114,    7,   54,   66,   39,  129,\n",
       "           3, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045],\n",
       "       [   2,  898,  384,    0,  898,   20,  252,    0,   39,  113,  261,\n",
       "         898,    3, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045],\n",
       "       [   2,  751, 2254, 1044,   12,    1,  346,   54,  147, 1335,    1,\n",
       "        9046,   53,   30,    3, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045],\n",
       "       [   2,  921,   47, 1163, 1041,  404, 9046, 2200,  130,  167,   88,\n",
       "         219,    3, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045],\n",
       "       [   2, 1463,  126,  143, 1885,   33,   24, 1623, 3979,  111,   12,\n",
       "        1403,  329,   43,    3, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045],\n",
       "       [   2, 1566, 2195, 2292,    0,  814,  179,   13,    3, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045],\n",
       "       [   2, 1724,  900, 3903,    0,   61,  128,    0,  237,    7,  288,\n",
       "           0, 1100, 1005,  790,    0,   11,  173,  141,  334,   70,  395,\n",
       "           0,   77,  235,  745,    5,   21,  334,   70,  229,    0,   79,\n",
       "         490,   12,    7,  288,    0,  406,  172,   42,   30,    0,   18,\n",
       "          52,  442,    4,  164,   31,   78,    4,  294,  397,  470,   43,\n",
       "           9,  170,    3, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc[:10][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 86), dtype=int32, numpy=\n",
       "array([[   2,   24,   55, 2397,  838,   37,  154, 3024,  100, 1839,   42,\n",
       "         689,    4,   23,   31,    9,    3, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045],\n",
       "       [   2, 2711,  611,  190,    0, 1369,   38,    0,  320,  286,  237,\n",
       "        9047,  931,  142,    0,  132,  419,   72, 9048,  354,    0,  321,\n",
       "        3069,    5,  320,   17,    5, 3544,   37,  488, 5135, 2676,    5,\n",
       "         272, 1652,    6,    9,    3, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045],\n",
       "       [   2,   48,  145,    0,  617,   47,  801,  611,    1,    0,   12,\n",
       "         386,    6,   24, 4455,    0,  340,  205,   12,  356,    0,  221,\n",
       "        9047, 1404,   24,  103,    0,  342,  362,   88, 9048,  430,  179,\n",
       "        9049,    4,   39,   85,   93,    9,  669,    7,  204,    4,  616,\n",
       "           0,   72,  352, 1250,  619,    0,  170,  145,    3, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_extend[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 86), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_mask[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=int32, numpy=\n",
       "array([0, 2, 3, 2, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 3, 0, 2, 0, 1, 2, 0, 1,\n",
       "       0, 1, 0, 1, 1, 3, 4, 1, 3, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 2, 1,\n",
       "       0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 3, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 2, 0, 0, 1, 0, 1, 0, 0, 2, 0, 1, 0, 3, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 3, 2, 3, 0, 2, 0, 2, 1, 1, 0, 2, 0, 1, 0,\n",
       "       0, 0, 3, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0], dtype=int32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_oov_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 86), dtype=int32, numpy=\n",
       "array([[   2,  154, 3024,    0, 2295, 3024,    0,  292,    1,  972,  135,\n",
       "          49,    3, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045],\n",
       "       [   2,  178,  305,   34,  283,    0,  192,   53, 3053,  187,    5,\n",
       "           3, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045],\n",
       "       [   2,   18,    4, 1250,    0,  856,  222,    0, 2492, 1292, 9046,\n",
       "           5,   30,    1,    0,  543,   41,    3, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045,\n",
       "        9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045, 9045]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 86), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_mask[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(lis):\n",
    "    return (len(lis), *lis[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred, padding_mask):\n",
    "#     print(1, get_shape(real), get_shape(pred), get_shape(padding_mask))\n",
    "#           1           (128, 86)        (85, 128, 9051)  (128, 86)\n",
    "\n",
    "    #  pred is list of list, max_len_y * [batch_sz, extend_vocabsize] (without argmax) real & mask [batch_sz, max_len_y]\n",
    "    #mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction=\"none\")\n",
    "    for t in range(real.shape[1]-1):\n",
    "        \n",
    "        loss_ = loss_object(real[:,t+1], pred[t])    # note this change 11.11 [batch_sz,]\n",
    "        mask = tf.cast(padding_mask[:,t+1], dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "        # print('loss_:', loss_)\n",
    "        loss_ = tf.reduce_mean(loss_, axis=0)  # batch-wise\n",
    "        loss += loss_\n",
    "        # print('loss and loss at each time step(batch sum)', loss, loss_)\n",
    "#     tf.print(\"loss:\",loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def coverage_loss(attn_dists, coverages, padding_mask):\n",
    "#     print(2, get_shape(attn_dists), get_shape(coverages), get_shape(padding_mask))\n",
    "#           2           (85, 128, 86, 1)       (85, 128, 86, 1)      (128, 86)\n",
    "    # attn_dists: [max_len_y, batch_sz, max_len_x, 1]\n",
    "    attn_dists = tf.squeeze(attn_dists, axis=-1)   # [max_len_y, batch_sz, max_len]\n",
    "    coverage = tf.zeros_like(attn_dists[0])  # shape (batch_size, max_len_x). Initial coverage is zero.\n",
    "    covlosses = []  # Coverage loss per decoder timestep. Will be list length max_dec_steps containing shape (batch_size).\n",
    "    padding_mask = tf.stack(padding_mask, 1)   # [max_len_y, batch_sz]\n",
    "    mask = tf.cast(padding_mask, dtype=attn_dists[1].dtype)\n",
    "    for i,a in enumerate(attn_dists):\n",
    "        covloss = tf.reduce_sum(tf.minimum(a, coverage), [1])  # calculate the coverage loss for this step\n",
    "        # update the coverage vector\n",
    "        coverage += a\n",
    "        covloss = covloss * mask[i,:]\n",
    "        covlosses.append(covloss)\n",
    "    loss = tf.reduce_sum(tf.reduce_mean(covlosses, axis=0))\n",
    "#     tf.print('coverage loss(batch sum):', loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tX7Dn2e5KEx6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tf.Tensor(\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1.], shape=(128,), dtype=float32)\n",
      "1 tf.Tensor(\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1.], shape=(128,), dtype=float32)\n",
      "2 tf.Tensor(\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1.], shape=(128,), dtype=float32)\n",
      "3 tf.Tensor(\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1.], shape=(128,), dtype=float32)\n",
      "4 tf.Tensor(\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1.], shape=(128,), dtype=float32)\n",
      "5 tf.Tensor(\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1.], shape=(128,), dtype=float32)\n",
      "6 tf.Tensor(\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1.], shape=(128,), dtype=float32)\n",
      "7 tf.Tensor(\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1.], shape=(128,), dtype=float32)\n",
      "8 tf.Tensor(\n",
      "[1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1.], shape=(128,), dtype=float32)\n",
      "9 tf.Tensor(\n",
      "[1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1.], shape=(128,), dtype=float32)\n",
      "10 tf.Tensor(\n",
      "[1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1.], shape=(128,), dtype=float32)\n",
      "11 tf.Tensor(\n",
      "[1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 0. 1. 1. 1.], shape=(128,), dtype=float32)\n",
      "12 tf.Tensor(\n",
      "[0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 0. 1. 1. 1.], shape=(128,), dtype=float32)\n",
      "13 tf.Tensor(\n",
      "[0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0.\n",
      " 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 0. 1. 1. 1.], shape=(128,), dtype=float32)\n",
      "14 tf.Tensor(\n",
      "[0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 0. 1. 1. 1.], shape=(128,), dtype=float32)\n",
      "15 tf.Tensor(\n",
      "[0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 0. 1. 1. 1.], shape=(128,), dtype=float32)\n",
      "16 tf.Tensor(\n",
      "[0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 0. 1. 1. 1.], shape=(128,), dtype=float32)\n",
      "17 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1.\n",
      " 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 0. 1. 1. 1.], shape=(128,), dtype=float32)\n",
      "18 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1.\n",
      " 1. 0. 1. 1. 0. 1. 1. 1.], shape=(128,), dtype=float32)\n",
      "19 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1.\n",
      " 1. 0. 1. 1. 0. 1. 1. 1.], shape=(128,), dtype=float32)\n",
      "20 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1.\n",
      " 1. 0. 1. 1. 0. 1. 1. 1.], shape=(128,), dtype=float32)\n",
      "21 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1.\n",
      " 1. 0. 1. 1. 0. 1. 0. 1.], shape=(128,), dtype=float32)\n",
      "22 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1.\n",
      " 1. 0. 1. 1. 0. 1. 0. 1.], shape=(128,), dtype=float32)\n",
      "23 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1.\n",
      " 1. 0. 1. 1. 0. 1. 0. 1.], shape=(128,), dtype=float32)\n",
      "24 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1.\n",
      " 1. 0. 1. 1. 0. 1. 0. 1.], shape=(128,), dtype=float32)\n",
      "25 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1.\n",
      " 1. 0. 0. 1. 0. 1. 0. 1.], shape=(128,), dtype=float32)\n",
      "26 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1.\n",
      " 1. 0. 0. 1. 0. 1. 0. 1.], shape=(128,), dtype=float32)\n",
      "27 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1.\n",
      " 1. 0. 0. 1. 0. 1. 0. 1.], shape=(128,), dtype=float32)\n",
      "28 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1.\n",
      " 1. 0. 0. 1. 0. 1. 0. 1.], shape=(128,), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.\n",
      " 1. 0. 0. 1. 0. 1. 0. 1.], shape=(128,), dtype=float32)\n",
      "30 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.\n",
      " 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.\n",
      " 1. 0. 0. 1. 0. 1. 0. 1.], shape=(128,), dtype=float32)\n",
      "31 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.\n",
      " 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 1.], shape=(128,), dtype=float32)\n",
      "32 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.\n",
      " 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 1.], shape=(128,), dtype=float32)\n",
      "33 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 1.], shape=(128,), dtype=float32)\n",
      "34 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 1.], shape=(128,), dtype=float32)\n",
      "35 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 1.], shape=(128,), dtype=float32)\n",
      "36 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 1.], shape=(128,), dtype=float32)\n",
      "37 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 1.], shape=(128,), dtype=float32)\n",
      "38 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1.], shape=(128,), dtype=float32)\n",
      "39 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1.], shape=(128,), dtype=float32)\n",
      "40 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "41 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "42 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "43 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "44 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "45 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "46 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "47 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "48 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "49 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "50 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "51 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "52 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "53 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "54 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "55 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "57 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "58 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "59 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "60 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "61 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "62 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "63 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "64 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "65 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "66 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "67 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "68 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "69 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "70 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "71 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "72 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "73 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "74 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "75 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "76 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "77 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "78 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "79 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "81 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "82 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "83 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n",
      "84 tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-130-cf9b61cca205>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mbatch_loss\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m train_one_batch('train',w2v_model,model,optimizer,dataset_oov_dict,st,enc,dec,enc_extend,enc_mask,\n\u001B[0;32m---> 28\u001B[0;31m                                          enc_oov_len, cov_loss_wt=cov_loss_wt, padding_mask=dec_mask)\n\u001B[0m",
      "\u001B[0;32m<ipython-input-130-cf9b61cca205>\u001B[0m in \u001B[0;36mtrain_one_batch\u001B[0;34m(mode, w2v_model, model, optimizer, oov_dict, st, inp, targ, enc_extended_inp, enc_pad_mask, batch_oov_len, cov_loss_wt, padding_mask)\u001B[0m\n\u001B[1;32m     13\u001B[0m         loss = loss_function(targ,final_dist,padding_mask)+cov_loss_wt*coverage_loss(attentions,coverages,\n\u001B[1;32m     14\u001B[0m                                                                                      padding_mask)\n\u001B[0;32m---> 15\u001B[0;31m         \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m         \u001B[0mbatch_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mloss\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtarg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "def train_one_batch(mode, w2v_model, model, optimizer, oov_dict, st, inp, targ, enc_extended_inp, enc_pad_mask,\n",
    "                    batch_oov_len, cov_loss_wt, padding_mask=None):\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = model.call_encoder(inp)\n",
    "        final_dist, attentions, coverages, _, _, _ = model(targ, enc_extended_inp, enc_pad_mask, batch_oov_len,\n",
    "                                                enc_output, enc_hidden, use_coverage=True,prev_coverage=None)\n",
    "        \n",
    "#         print(final_dist[0].shape, attentions[0].shape, coverages[0].shape)\n",
    "#         (128, 9051) (128, 86, 1) (128, 86, 1)\n",
    "#         print(len(final_dist), len(attentions), len(coverages))\n",
    "#         85 85 85\n",
    "        \n",
    "        loss = loss_function(targ,final_dist,padding_mask)+cov_loss_wt*coverage_loss(attentions,coverages,\n",
    "                                                                                     padding_mask)\n",
    "        raise\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        variables = model.encoder.trainable_variables + model.attention.trainable_variables + \\\n",
    "                        model.decoder.trainable_variables + model.pointer.trainable_variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        return batch_loss\n",
    "train_one_batch('train',w2v_model,model,optimizer,dataset_oov_dict,st,enc,dec,enc_extend,enc_mask,\n",
    "                                         enc_oov_len, cov_loss_wt=cov_loss_wt, padding_mask=dec_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "49dm7KLs9LDb"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    calculate encoded output and hidden state from encoder input and initialized encoder hidden\n",
    "    batch by batch and paragraph by paragraph\n",
    "    input is [batch_sz,max_len_x] encoder hidden is [batch_sz, enc_units]\n",
    "    output is [batch_sz, max_len_x, enc_units]\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_units, batch_sz, embedding_matrix):\n",
    "        super(Encoder, self).__init__()\n",
    "        vocab_size, embedding_dim = embedding_matrix.shape[0], embedding_matrix.shape[1]\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units // 2\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=False)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.bidirectional_gru = tf.keras.layers.Bidirectional(self.gru)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        # [batch_sz,max_train_x,embedding_dim]\n",
    "        x = self.embedding(x)\n",
    "        # output is [batch_sz, max_train_x, enc_units] enc_hidden after concat is [batch_sz, enc_units]\n",
    "        output, forward_state, backward_state = self.bidirectional_gru(x, initial_state=[hidden, hidden])\n",
    "        enc_hidden = tf.keras.layers.concatenate([forward_state, backward_state],axis=-1)\n",
    "        return output, enc_hidden\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    calculate attention and coverage from dec_hidden enc_output and prev_coverage\n",
    "    one dec_hidden(word) by one dec_hidden\n",
    "    dec_hidden or query is [batch_sz, enc_unit], enc_output or values is [batch_sz, max_train_x, enc_units],\n",
    "    prev_coverage is [batch_sz, max_len_x, 1]\n",
    "    dec_hidden is initialized as enc_hidden, prev_coverage is initialized as None\n",
    "    output context_vector [batch_sz, enc_units] attention_weights & coverage [batch_sz, max_len_x, 1]\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W_h = tf.keras.layers.Dense(units)\n",
    "        self.W_s = tf.keras.layers.Dense(units)\n",
    "        self.W_c = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, dec_hidden, enc_output, enc_pad_mask, use_coverage, prev_coverage):\n",
    "\n",
    "        # prev_coverage [batch_sz, max_len_x, 1]\n",
    "        # 11.07 add enc_pad_mask [batch_sz, max_len_x] to mask attention\n",
    "        # query or dec_hidden [batch_sz, enc_units], values or enc_output [batch_sz, max_len, enc_units]\n",
    "        # hidden_with_time_axis [batch_size, 1, enc_units]\n",
    "        hidden_with_time_axis = tf.expand_dims(dec_hidden, 1)\n",
    "\n",
    "        if use_coverage and prev_coverage is not None:\n",
    "            # self.W_s(values) [batch_sz, max_len, units] self.W_h(hidden_with_time_axis) [batch_sz, 1, units]\n",
    "            # self.W_c(prev_coverage) [batch_sz, max_len, units]  score [batch_sz, max_len, 1]\n",
    "            score = self.V(tf.nn.tanh(self.W_s(enc_output) + self.W_h(hidden_with_time_axis) + self.W_c(prev_coverage)))\n",
    "            # attention_weights shape (batch_size, max_len, 1)\n",
    "            mask = tf.cast(enc_pad_mask, dtype=score.dtype)\n",
    "            masked_score = tf.squeeze(score, axis=-1) * mask\n",
    "            masked_score = tf.expand_dims(masked_score, axis=2)\n",
    "            attention_weights = tf.nn.softmax(masked_score, axis=1)\n",
    "\n",
    "            coverage = attention_weights + prev_coverage\n",
    "\n",
    "        else:\n",
    "            # self.W1(values) [batch_sz, max_len, units] self.W2(hidden_with_time_axis): [batch_sz, 1, units]\n",
    "            # score [batch_sz, max_len, 1]\n",
    "            score = self.V(tf.nn.tanh(self.W_s(enc_output) + self.W_h(hidden_with_time_axis)))\n",
    "            mask = tf.cast(enc_pad_mask, dtype=score.dtype)\n",
    "            masked_score = tf.squeeze(score, axis=-1) * mask\n",
    "            masked_score = tf.expand_dims(masked_score, axis=2)\n",
    "            attention_weights = tf.nn.softmax(masked_score, axis=1)\n",
    "\n",
    "            if use_coverage:\n",
    "                coverage = attention_weights\n",
    "\n",
    "        # [batch_sz, max_len, enc_units]\n",
    "        context_vector = attention_weights * enc_output\n",
    "        # [batch_sz, enc_units]\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights, coverage\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    calculate output before pointer generator network\n",
    "    input dec_inp [batch_sz, 1], hidden [batch_sz, enc_units], context_vector [batch_sz, enc_units]\n",
    "    output dec_inp_context [batch_sz,1,embedding_dim+enc_units] dec_pred [batch_size,vocab_size] dec_hidden [batch_sz,dec_units]\n",
    "    \"\"\"\n",
    "    def __init__(self, dec_units, batch_sz, embedding_matrix):\n",
    "        super(Decoder, self).__init__()\n",
    "        vocab_size, embedding_dim = embedding_matrix.shape[0], embedding_matrix.shape[1]\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=False)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size, activation=tf.keras.activations.softmax)\n",
    "\n",
    "    def call(self, dec_inp, dec_hidden, context_vector):\n",
    "        # context_vector[batch_sz, enc_units]\n",
    "        # dec_hidden [batch_sz, dec_units] NOTE dec_units == enc_units\n",
    "        # dec_inp shape [batch_size, 1, embedding_dim]\n",
    "        dec_inp = self.embedding(dec_inp)\n",
    "        # dec_inp shape [batch_sz, 1, embedding_dim + enc_units]\n",
    "        dec_inp_context = tf.concat([tf.expand_dims(context_vector, 1), dec_inp], axis=-1)\n",
    "        # output [batch_sz, 1, dec_units] state [batch_sz, dec_units]\n",
    "        output, dec_hidden = self.gru(dec_inp_context)\n",
    "        # output shape [batch_size, dec_units]\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        # print('output deduced by dec_hidden', tf.math.reduce_sum(output-dec_hidden)) they are same!!!\n",
    "        # dec_inp shape [batch_size, vocab_size]\n",
    "        dec_pred = self.fc(output)\n",
    "        return dec_inp_context, dec_pred, dec_hidden\n",
    "\n",
    "\n",
    "class Pointer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    calculate Pgen\n",
    "    input context_vector [batch_sz,enc_units] dec_hidden [batch_sz,dec_units] dec_inp_context [batch_sz,1,embedding_dim+enc_units]\n",
    "    output scaler pgen\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Pointer, self).__init__()\n",
    "        self.w_s_reduce = tf.keras.layers.Dense(1)\n",
    "        self.w_i_reduce = tf.keras.layers.Dense(1)\n",
    "        self.w_c_reduce = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, context_vector, dec_hidden, dec_inp):\n",
    "        # change dec_inp_context to [batch_sz,embedding_dim+enc_units]\n",
    "        dec_inp = tf.squeeze(dec_inp, axis=1)\n",
    "        pgen = tf.nn.sigmoid(self.w_s_reduce(dec_hidden) + self.w_c_reduce(context_vector) + self.w_i_reduce(dec_inp))\n",
    "        return pgen\n",
    "\n",
    "class PGN(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    create pgn model\n",
    "    input\n",
    "    output\n",
    "    \"\"\"\n",
    "    def __init__(self, gru_units, att_units, batch_sz, embedding_matrix):\n",
    "        super(PGN, self).__init__()\n",
    "        vocab_size, embedding_dim = embedding_matrix.shape[0], embedding_matrix.shape[1]\n",
    "        self.enc_units = gru_units\n",
    "        self.dec_units = gru_units\n",
    "        self.att_units = att_units\n",
    "        self.encoder = Encoder(self.enc_units, batch_sz, embedding_matrix)\n",
    "        self.attention = BahdanauAttention(self.att_units)\n",
    "        self.decoder = Decoder(self.dec_units, batch_sz, embedding_matrix)\n",
    "        self.pointer = Pointer()\n",
    "\n",
    "    def call_encoder(self, enc_inp):\n",
    "        enc_hidden = self.encoder.initialize_hidden_state()    # [batch_sz, enc_units]\n",
    "        enc_output, enc_hidden = self.encoder(enc_inp, enc_hidden)   # [batch_sz, max_train_x, enc_units], [batch_sz, enc_units]\n",
    "        return enc_output, enc_hidden\n",
    "\n",
    "    # NOTE dec_inp [batch_sz, max_train_y]\n",
    "    def call(self, dec_inp, enc_extended_inp, enc_pad_mask, batch_oov_len, enc_output, dec_hidden,\n",
    "             use_coverage=True, prev_coverage=None, prediction=False):\n",
    "        predictions = []\n",
    "        attentions = []\n",
    "        coverages = []\n",
    "        p_gens = []\n",
    "\n",
    "        # # initiate_hidden and context_vector for pgn\n",
    "        # enc_hidden = self.encoder.initialize_hidden_state()\n",
    "        # enc_output, enc_hidden = self.encoder(enc_inp, enc_hidden)  # same as above\n",
    "        # # initiate coverage to None or whatever was passed in\n",
    "        # dec_hidden = enc_hidden\n",
    "        # # initiate context_vector and coverage_ret\n",
    "\n",
    "        context_vector, attention_weights, coverage_ret = self.attention(dec_hidden,enc_output,enc_pad_mask,\n",
    "                                                                         use_coverage,prev_coverage)\n",
    "\n",
    "        if prediction:\n",
    "            decode_steps = dec_inp.shape[1]\n",
    "        else:\n",
    "            decode_steps = dec_inp.shape[1] - 1\n",
    "\n",
    "        for t in range(decode_steps):  # 11.11 iterate over time step max_len_y - 1 !!!!\n",
    "\n",
    "            # attentions.append(attention_weights)\n",
    "            # decoder takes dec_inp, dec_hidden, context_vector, dec_inp shape [batch_size, 1]\n",
    "            # dec_hidden [batch_sz, dec_units] NOTE dec_units == enc_units, context_vector[batch_sz, enc_units]\n",
    "            # decoder gives dec_pred [batch_size,vocab_size] dec_hidden [batch_sz,dec_units]\n",
    "            # using teaching force!!!\n",
    "            attentions.append(attention_weights)\n",
    "            coverages.append(coverage_ret)\n",
    "            \n",
    "            dec_inp_context, dec_pred, dec_hidden = self.decoder(tf.expand_dims(dec_inp[:, t], 1), dec_hidden,\n",
    "                                                                 context_vector)\n",
    "            # attention takes dec_hidden, enc_output, use_coverage = True, prev_coverage = None\n",
    "            # attention gives context_vector, attention_weights, coverage\n",
    "            if not prediction:\n",
    "                context_vector, attention_weights, coverage_ret = self.attention(dec_hidden, enc_output,\n",
    "                                                            enc_pad_mask,use_coverage,coverage_ret)\n",
    "            # pointer takes context_vector, dec_hidden, dec_inp\n",
    "            pgen = self.pointer(context_vector, dec_hidden, dec_inp_context)\n",
    "            predictions.append(dec_pred)\n",
    "            p_gens.append(pgen)\n",
    "\n",
    "\n",
    "        # enc_extended_input [batch_sz, max_len_x] predictions [max_len_y, batch_sz, vocab_size]\n",
    "        # attentions [max_len_y, batch_sz, max_len_x, 1] p_gens [max_len_y,]\n",
    "        final_dist = self._calc_final_dist(enc_extended_inp, predictions, attentions, p_gens, batch_oov_len)\n",
    "        # change shape of final_dist from [max_len_y, batch_sz, extend_vocab_size]\n",
    "        # to [batch_sz, max_len_y, extend_vocab_size]\n",
    "        #final_dist = tf.transpose(final_dist, [1, 0, 2])\n",
    "\n",
    "        return final_dist, attentions, coverages, dec_hidden, context_vector, p_gens\n",
    "\n",
    "\n",
    "    def _calc_final_dist(self, enc_extended_inp, vocab_dists, attn_dists, p_gens, batch_oov_len):\n",
    "        \"\"\"\n",
    "        Calculate the final distribution, for the pointer-generator model\n",
    "        Args:\n",
    "        vocab_dists, prediction of decoder List length max_dec_steps of (batch_sz, vocab_size) array.\n",
    "                    The words are in the order they appear in the vocabulary file.\n",
    "        attn_dists: The attention distributions. List length max_dec_steps of (batch_size, max_train_x, 1) array.\n",
    "        _enc_batch_extend_vocab, tokenized enc input (batch_sz, max_train_x) with pgn the in-article oov word is\n",
    "        tonkenized with extended index.\n",
    "        Returns:\n",
    "        final_dists: The final distributions. List length max_dec_steps of (batch_size, extended_vocab_size) arrays.\n",
    "        \"\"\"\n",
    "#       enc_extended_inp.shape, vocab_dists[0].shape, attn_dists[0].shape, p_gens[0].shape, batch_oov_len.shape\n",
    "#       (128, 86)               (128, 9047)           (128, 86, 1)         (128, 1)         (128,)\n",
    "#         print(len(attn_dists), attn_dists[0].shape)\n",
    "#         85 (128, 86, 1)\n",
    "\n",
    "        max_len_y, batch_sz, vocab_size = len(vocab_dists), vocab_dists[0].shape[0], vocab_dists[0].shape[1]\n",
    "        attn_dists = tf.squeeze(attn_dists, axis = -1) # change to max_dec_steps of (batch_size, max_train_x) array\n",
    "        batch_oov_len = tf.reduce_max(batch_oov_len)  # the maximum (over the batch) size of the extended vocabulary\n",
    "        \n",
    "#         print(attn_dists.shape)\n",
    "#         (85, 128, 86)\n",
    "        \n",
    "        # p_gens = tf.convert_to_tensor(p_gens)\n",
    "        vocab_dists = [p_gen * dist for (p_gen, dist) in zip(p_gens, vocab_dists)]\n",
    "        attn_dists = [(1 - p_gen) * dist for (p_gen, dist) in zip(p_gens, attn_dists)]\n",
    "        # to substitute above code\n",
    "        # def weight_cross(p_gens, vocab_dists):\n",
    "        #     list_like = tf.TensorArray(dtype=tf.float32,size=1,dynamic_size=True,clear_after_read=False)\n",
    "        #     for i in range(len(p_gens)):\n",
    "        #         list_like = list_like.write(i, p_gens[i] * vocab_dists[i])\n",
    "        #     return list_like.stack()\n",
    "        # vocab_dists = weight_cross(p_gens, vocab_dists)\n",
    "        # attn_dists = weight_cross((1-p_gens), attn_dists)\n",
    "\n",
    "        # Concatenate some zeros to each vocabulary dist, to hold the probabilities for in-article OOV words\n",
    "        extended_vocab_size = vocab_size + batch_oov_len\n",
    "        extra_zeros = tf.zeros((batch_sz, batch_oov_len))\n",
    "        vocab_dists_extended = [tf.concat(axis=1, values=[dist, extra_zeros]) for dist in vocab_dists]\n",
    "\n",
    "        # extra_zeros = tf.zeros((max_len_y, batch_sz, batch_oov_len))\n",
    "        # vocab_dists_extended = tf.concat(axis=2, values=[vocab_dists,extra_zeros])\n",
    "        # list length max_dec_steps of shape (batch_size, extended_vsize) [max_len_y, batch_sz, extended_vsize]\n",
    "        # vocab_dists_extended = [tf.concat(axis=1, values=[dist, extra_zeros]) for dist in vocab_dists]\n",
    "        # to substitute above code\n",
    "\n",
    "\n",
    "        # Project the values in the attention distributions onto the appropriate entries in the final distributions\n",
    "        # This means that if a_i = 0.1 and the ith encoder word is w, and w has index 500 in the vocabulary,\n",
    "        # then we add 0.1 onto the 500th entry of the final distribution\n",
    "        # This is done for each decoder timestep.\n",
    "        # This is fiddly; we use tf.scatter_nd to do the projection\n",
    "        batch_nums = tf.range(0, limit=batch_sz)  # shape (batch_size)\n",
    "        batch_nums = tf.expand_dims(batch_nums, 1)  # shape (batch_size, 1)\n",
    "#         print(batch_nums)\n",
    "        max_len_x = tf.shape(enc_extended_inp)[1]  # number of states we attend over\n",
    "        batch_nums = tf.tile(batch_nums, [1, max_len_x])  # shape (batch_size, max_train_x)\n",
    "#         print(batch_nums)\n",
    "#         [[  0   0   0 ...   0   0   0]\n",
    "#          [  1   1   1 ...   1   1   1]\n",
    "#          [  2   2   2 ...   2   2   2]\n",
    "#          ...\n",
    "#          [125 125 125 ... 125 125 125]\n",
    "#          [126 126 126 ... 126 126 126]\n",
    "#          [127 127 127 ... 127 127 127]]\n",
    "#         raise\n",
    "        indices = tf.stack((batch_nums, enc_extended_inp), axis=2)  # shape (batch_size, max_train_x, 2)\n",
    "#         enc_extended_inp (128, 84)\n",
    "#         print(indices)\n",
    "#         [[[   0    2]\n",
    "#           [   0   24]\n",
    "#           [   0   55]\n",
    "#           ...\n",
    "#           [   0 9045]\n",
    "#           [   0 9045]\n",
    "#           [   0 9045]]\n",
    "\n",
    "#          [[   1    2]\n",
    "#           [   1 2711]\n",
    "#           [   1  611]\n",
    "#           ...\n",
    "#           [   1 9045]\n",
    "#           [   1 9045]\n",
    "#           [   1 9045]]\n",
    "\n",
    "#          [[ 127    2]\n",
    "#           [ 127 1187]\n",
    "#           [ 127    1]\n",
    "#           ...\n",
    "#           [ 127 9045]\n",
    "#           [ 127 9045]\n",
    "#           [ 127 9045]]], shape=(128, 86, 2)\n",
    "#         raise\n",
    "        \n",
    "        shape = (batch_sz, extended_vocab_size)\n",
    "        # list length max_dec_steps (batch_size, extended_vocab_size)\n",
    "        attn_dists_projected = [tf.scatter_nd(indices, copy_dist, shape) for copy_dist in attn_dists]\n",
    "        # substitute above code\n",
    "        # temp = tf.TensorArray(dtype=tf.float32,size=1,dynamic_size=True,clear_after_read=False)\n",
    "        # for i in range(p_gens.shape[0]):  # which equal to max_len_y\n",
    "        #     temp = temp.write(i, tf.scatter_nd(indices, attn_dists[i], shape))\n",
    "        # attn_dists_projected = temp.stack()\n",
    "\n",
    "        # Add the vocab distributions and the copy distributions together to get the final distributions\n",
    "        # final_dists is a list length max_dec_steps; each entry is a tensor shape (batch_size, extended_vsize) giving\n",
    "        # the final distribution for that decoder timestep\n",
    "        # Note that for decoder timesteps and examples corresponding to a [PAD] token, this is junk - ignore.\n",
    "        final_dists = [vocab_dist+copy_dist for (vocab_dist,copy_dist) in zip(vocab_dists_extended,\n",
    "                                                                              attn_dists_projected)]\n",
    "        # to substitute code above\n",
    "        # final_dists = tf.add(vocab_dists_extended,attn_dists_projected)\n",
    "\n",
    "        return final_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s9hoa6e8I2yB"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gru_unites' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-a0c57a4671af>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPGN\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgru_unites\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattn_units\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minp_vocabulary_matrix\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'gru_unites' is not defined"
     ]
    }
   ],
   "source": [
    "model = PGN(gru_unites, attn_units, batch_size, np.array(inp_vocabulary_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}