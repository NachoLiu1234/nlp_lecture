{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import jieba\n",
    "import time\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocabulary_number, vocabulary_dimension, vocabulary_matrix, gru_unites, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.gru_unites = gru_unites\n",
    "        self.embedding = tf.keras.layers.Embedding(vocabulary_number, vocabulary_dimension, \n",
    "#                                                weights=vocabulary_matrix, trainable=False\n",
    "                                                  )\n",
    "        self.gru = tf.keras.layers.GRU(gru_unites, return_sequences=True, return_state=True)\n",
    "    \n",
    "    def call(self, inp, hidden):\n",
    "        x = self.embedding(inp)\n",
    "        x, hidden = self.gru(x, initial_state=hidden)\n",
    "        return x, hidden\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.gru_unites))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        self.w1 = tf.keras.layers.Dense(units)\n",
    "        self.w2 = tf.keras.layers.Dense(units)\n",
    "        self.v = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, dec_hidden, enc_output):\n",
    "        hidden_with_time_axis = tf.expand_dims(dec_hidden, 1)\n",
    "        score = self.v(tf.nn.tanh(self.w1(enc_output) + self.w2(hidden_with_time_axis)))\n",
    "        attn_dist = tf.nn.softmax(score, axis=1)\n",
    "#         attn_dist = tf.expand_dims(attn_dist, axis=2)\n",
    "        context_vector = attn_dist * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attn_dist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocabulary_number, vocabulary_dimension, vocabulary_matrix, unites, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocabulary_number, vocabulary_dimension, \n",
    "                                                  weights=[vocabulary_matrix], trainable=False)\n",
    "        self.gru = tf.keras.layers.GRU(unites, return_sequences=True, return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocabulary_number, activation=tf.keras.activations.softmax)\n",
    "        \n",
    "        # BahdanauAttention\n",
    "        self.w1 = tf.keras.layers.Dense(unites)\n",
    "        self.w2 = tf.keras.layers.Dense(unites)\n",
    "        self.v = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, x, dec_hidden, enc_output):\n",
    "        # dec_hidden shape == (batch_size, hidden size)\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "\n",
    "        hidden_with_time_axis = tf.expand_dims(dec_hidden, 1)\n",
    "        score = tf.nn.tanh(self.w1(enc_output) + self.w2(hidden_with_time_axis))\n",
    "        attention_weights = tf.nn.softmax(self.v(score), axis=1)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "#         print(x.shape)\n",
    "#         print(x)\n",
    "#         e = tf.keras.layers.Embedding(len(targ_lang.word2idx), vocabulary_dimension)\n",
    "#         print(e(x))\n",
    "#         self.embedding(1)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * max_length, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageIndex():\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "        \n",
    "        self.create_index()\n",
    "        \n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "            self.vocab.update(phrase)\n",
    "        \n",
    "        self.word2idx['<pad>'] = 0\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1\n",
    "        \n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<start>',\n",
       "  '收盘',\n",
       "  '前',\n",
       "  '的',\n",
       "  '最后',\n",
       "  '三分钟',\n",
       "  '交易',\n",
       "  '重新',\n",
       "  '打开',\n",
       "  '（',\n",
       "  '按照',\n",
       "  '规定',\n",
       "  '，',\n",
       "  '尾盘',\n",
       "  '三分钟',\n",
       "  '，',\n",
       "  '可转债',\n",
       "  '不设',\n",
       "  '涨跌幅',\n",
       "  '限制',\n",
       "  '）',\n",
       "  '，',\n",
       "  '泰晶',\n",
       "  '转债',\n",
       "  '最终',\n",
       "  '跌幅',\n",
       "  '扩大',\n",
       "  '至',\n",
       "  '47.68%',\n",
       "  '。',\n",
       "  '<end>'],\n",
       " ['<start>',\n",
       "  'The',\n",
       "  'trading',\n",
       "  'was',\n",
       "  'reopened',\n",
       "  'in',\n",
       "  'the',\n",
       "  'last',\n",
       "  'three',\n",
       "  'minutes',\n",
       "  'before',\n",
       "  'the',\n",
       "  'close',\n",
       "  '(',\n",
       "  'according',\n",
       "  'to',\n",
       "  'the',\n",
       "  'regulations',\n",
       "  ',',\n",
       "  'there',\n",
       "  'is',\n",
       "  'no',\n",
       "  'limit',\n",
       "  'for',\n",
       "  'the',\n",
       "  'rise',\n",
       "  'and',\n",
       "  'fall',\n",
       "  'of',\n",
       "  'convertible',\n",
       "  'bonds',\n",
       "  'for',\n",
       "  'three',\n",
       "  'minutes',\n",
       "  'at',\n",
       "  'the',\n",
       "  'end',\n",
       "  'of',\n",
       "  'the',\n",
       "  'day',\n",
       "  ')',\n",
       "  ',',\n",
       "  'and',\n",
       "  'the',\n",
       "  'final',\n",
       "  'decline',\n",
       "  'of',\n",
       "  'Taijing',\n",
       "  'Convertible',\n",
       "  'Bonds',\n",
       "  'expanded',\n",
       "  'to',\n",
       "  '47.68%',\n",
       "  '.',\n",
       "  '<end>'])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese = \"\"\"5月7日，泰晶转债开盘暴跌30%而暂停交易。\n",
    "收盘前的最后三分钟交易重新打开（按照规定，尾盘三分钟，可转债不设涨跌幅限制），泰晶转债最终跌幅扩大至47.68%。\n",
    "此次暴跌源于5月6日晚间泰晶科技发布的“关于提前赎回泰晶转债的提示性公告”，再结合此前再升转债宣布赎回时的暴跌，反衬出当前可转债市场的非理性狂热。\n",
    "可转债就是可以选择转股的债券。\n",
    "一般期限为6年左右，可转股日为发行后6个月，具体要看各家发行的报告。\n",
    "发行后半年投资者可以选择转股，按照票面金额来转股，例如100元的票面，转股价格20元，当前股价为40元，则对应转股价值为200元。\n",
    "若转债价格高于200元，就会产生正的转股溢价率。\n",
    "转债的纯债价值可以对投资者起到保护作用，一旦股价下跌过多，没有了转股价值，投资者至少可以以纯债的形式持有。\"\"\".split('\\n')\n",
    "english = \"\"\"On May 7, the opening of Taijing Convertible Bonds plunged 30% and trading was suspended. \n",
    "The trading was reopened in the last three minutes before the close (according to the regulations, there is no limit for the rise and fall of convertible bonds for three minutes at the end of the day), and the final decline of Taijing Convertible Bonds expanded to 47.68%. \n",
    "This plunge originated from the \"Informative Announcement on Redemption of Taijing Convertible Bonds in Advance\" released by Taijing Technology on the evening of May 6, combined with the plunge when the previous redemption of convertible bonds was announced to redeem, reflecting the current convertible bonds Irrational fanaticism in the market.\n",
    "Convertible bonds are bonds that can be converted into shares. \n",
    "The general term is about 6 years, and the conversion date is 6 months after the issue, depending on the report issued by each company. \n",
    "Investors can choose to convert shares half a year after the issuance and convert the shares according to the face value. \n",
    "For example, the face value of 100 yuan, the conversion price of 20 yuan, the current stock price of 40 yuan, then the corresponding value of 200 yuan. \n",
    "If the convertible bond price is higher than 200 yuan, a positive conversion premium rate will be generated. \n",
    "The pure debt value of convertible bonds can play a protective role for investors. Once the stock price falls too much, there is no conversion value, and investors can at least hold it in the form of pure debt.\"\"\".split('\\n')\n",
    "chinese = [['<start>'] + jieba.lcut(el) + ['<end>'] for el in chinese]\n",
    "english = [['<start>'] + [e for e in jieba.lcut(el) if e != ' '] + ['<end>'] for el in english]\n",
    "chinese[1], english[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([65,\n",
       "  66,\n",
       "  60,\n",
       "  109,\n",
       "  4,\n",
       "  5,\n",
       "  105,\n",
       "  93,\n",
       "  38,\n",
       "  2,\n",
       "  96,\n",
       "  17,\n",
       "  26,\n",
       "  113,\n",
       "  5,\n",
       "  26,\n",
       "  40,\n",
       "  118,\n",
       "  85,\n",
       "  68,\n",
       "  103,\n",
       "  26,\n",
       "  30,\n",
       "  67,\n",
       "  82,\n",
       "  56,\n",
       "  110,\n",
       "  97,\n",
       "  36,\n",
       "  78,\n",
       "  39],\n",
       " [91,\n",
       "  107,\n",
       "  21,\n",
       "  64,\n",
       "  83,\n",
       "  2,\n",
       "  29,\n",
       "  82,\n",
       "  114,\n",
       "  57,\n",
       "  23,\n",
       "  29,\n",
       "  75,\n",
       "  49,\n",
       "  111,\n",
       "  102,\n",
       "  29,\n",
       "  14,\n",
       "  8,\n",
       "  118,\n",
       "  96,\n",
       "  84,\n",
       "  51,\n",
       "  108,\n",
       "  29,\n",
       "  137,\n",
       "  39,\n",
       "  42,\n",
       "  46,\n",
       "  37,\n",
       "  58,\n",
       "  108,\n",
       "  114,\n",
       "  57,\n",
       "  38,\n",
       "  29,\n",
       "  6,\n",
       "  46,\n",
       "  29,\n",
       "  22,\n",
       "  76,\n",
       "  8,\n",
       "  39,\n",
       "  29,\n",
       "  90,\n",
       "  128,\n",
       "  46,\n",
       "  45,\n",
       "  30,\n",
       "  109,\n",
       "  59,\n",
       "  102,\n",
       "  50,\n",
       "  116,\n",
       "  53])"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_lang = LanguageIndex(chinese)\n",
    "targ_lang = LanguageIndex(english)\n",
    "\n",
    "chinese = [[inp_lang.word2idx[w] for w in s] for s in chinese]\n",
    "english = [[targ_lang.word2idx[w] for w in s] for s in english]\n",
    "chinese[1], english[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "max_length_inp, max_length_tar = max_length(chinese), max_length(english)\n",
    "\n",
    "input_tensor = tf.keras.preprocessing.sequence.pad_sequences(chinese, \n",
    "                                                                 maxlen=max_length_inp,\n",
    "                                                                 padding='post')\n",
    "target_tensor = tf.keras.preprocessing.sequence.pad_sequences(chinese, \n",
    "                                                                  maxlen=max_length_tar, \n",
    "                                                                  padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 65,  92,  84,  95,   1,  26,  30,  67, 116,  44,  24,  69,  10,\n",
       "         105,  78,  39,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0],\n",
       "        [ 65,  66,  60, 109,   4,   5, 105,  93,  38,   2,  96,  17,  26,\n",
       "         113,   5,  26,  40, 118,  85,  68, 103,  26,  30,  67,  82,  56,\n",
       "         110,  97,  36,  78,  39,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0],\n",
       "        [ 65,  86,  44, 119,  92,  84,  41,   1,  15,  30,  91, 120, 109,\n",
       "          37,  87, 106,  29,  30,  67, 109,  83,  42,  28,  26, 115,  58,\n",
       "         117,  90,  67,  49,  29,  71, 109,  44,  26,  21, 112,  63,  40,\n",
       "         101, 109,  94,  12,  78,  39],\n",
       "        [ 65,  40,  88, 111,  27,  76, 109,  73,  78,  39,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0],\n",
       "        [ 65,  45,  43, 107,  41,  53,   6,  26, 104,  19, 107,  33,  20,\n",
       "          41,  18,  84,  26,  61,  74,  64,  70,  33, 109,  89,  78,  39,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0],\n",
       "        [ 65,  33,  20,  57,  72, 111,  27,  76,  26,  96,  75,  98,  31,\n",
       "          76,  26,  77,  51,  32, 109,  75,  26,  76,   7, 100,  32,  26,\n",
       "          63,  22, 107,  55,  32,  26,  50,  23,  76,   3, 107, 114,  32,\n",
       "          78,  39,   0,   0,   0,   0],\n",
       "        [ 65,  11,  67,   7,  48, 114,  32,  26, 102,  13,  54,  52, 109,\n",
       "          76,  16, 108,  78,  39,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0],\n",
       "        [ 65,  67, 109,  62,   3, 111,  14,  72,  99,   9,  79,  26,  34,\n",
       "          22,  47,  25,  26,  35,   8,  76,   3,  26,  72,  80, 111,  46,\n",
       "         109,  59,  81,  78,  39,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0]], dtype=int32),\n",
       " array([[ 65,  92,  84,  95,   1,  26,  30,  67, 116,  44,  24,  69,  10,\n",
       "         105,  78,  39,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0],\n",
       "        [ 65,  66,  60, 109,   4,   5, 105,  93,  38,   2,  96,  17,  26,\n",
       "         113,   5,  26,  40, 118,  85,  68, 103,  26,  30,  67,  82,  56,\n",
       "         110,  97,  36,  78,  39,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0],\n",
       "        [ 65,  86,  44, 119,  92,  84,  41,   1,  15,  30,  91, 120, 109,\n",
       "          37,  87, 106,  29,  30,  67, 109,  83,  42,  28,  26, 115,  58,\n",
       "         117,  90,  67,  49,  29,  71, 109,  44,  26,  21, 112,  63,  40,\n",
       "         101, 109,  94,  12,  78,  39,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0],\n",
       "        [ 65,  40,  88, 111,  27,  76, 109,  73,  78,  39,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0],\n",
       "        [ 65,  45,  43, 107,  41,  53,   6,  26, 104,  19, 107,  33,  20,\n",
       "          41,  18,  84,  26,  61,  74,  64,  70,  33, 109,  89,  78,  39,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0],\n",
       "        [ 65,  33,  20,  57,  72, 111,  27,  76,  26,  96,  75,  98,  31,\n",
       "          76,  26,  77,  51,  32, 109,  75,  26,  76,   7, 100,  32,  26,\n",
       "          63,  22, 107,  55,  32,  26,  50,  23,  76,   3, 107, 114,  32,\n",
       "          78,  39,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0],\n",
       "        [ 65,  11,  67,   7,  48, 114,  32,  26, 102,  13,  54,  52, 109,\n",
       "          76,  16, 108,  78,  39,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0],\n",
       "        [ 65,  67, 109,  62,   3, 111,  14,  72,  99,   9,  79,  26,  34,\n",
       "          22,  47,  25,  26,  35,   8,  76,   3,  26,  72,  80, 111,  46,\n",
       "         109,  59,  81,  78,  39,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0]], dtype=int32))"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_dimension = 20\n",
    "inp_vocabulary_matrix = np.random.rand(len(inp_lang.word2idx), vocabulary_dimension)\n",
    "targ_vocabulary_matrix = np.random.rand(len(targ_lang.word2idx), vocabulary_dimension)\n",
    "unites = 128\n",
    "batch_size = 4\n",
    "lr = 0.0001\n",
    "encoder = Encoder(len(inp_lang.word2idx), vocabulary_dimension, inp_vocabulary_matrix, unites, batch_size)\n",
    "decoder = Decoder(len(targ_lang.word2idx), vocabulary_dimension, targ_vocabulary_matrix, unites, batch_size)\n",
    "optimizer = tf.keras.optimizers.Adam(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor, target_tensor)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((4, 45), (4, 57)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training, time cost: 0.8394513130187988\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "t1 = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "\n",
    "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * batch_size, 1)\n",
    "\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_output)\n",
    "                del attention_weights\n",
    "\n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "                \n",
    "    total_loss += loss / targ.shape[1]\n",
    "    variables = encoder.variables + decoder.variables\n",
    "    \n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "print(f'finished training, time cost: {time.time() - t1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
